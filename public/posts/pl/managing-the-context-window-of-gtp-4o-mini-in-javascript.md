---
title: Zarzdzanie context window modelu GPT-4o-mini w JavaScript
createdAt: 26-02-2025
tags: gpt-4o-mini, openai, javascript, nlp, node.js
description: Zanurzmy si w temat context window modelu GPT-4o-mini i odkryjmy strategie oraz narzdzia do zarzdzania nim w JavaScript, budujc keyword extractor zdolny do obsugi du偶ych iloci danych tekstowych.
---

# Zarzdzanie context window modelu GPT-4o-mini w JavaScript

GPT-4o-mini to pot偶ny, szybki i przystpny cenowo model jzykowy, kt贸ry mo偶e by u偶ywany do szerokiego zakresu zada w r贸偶nych jzykach. Context window tego modelu wynosi 128k token贸w.

Mo偶na by pomyle, 偶e to du偶o, ale gdy we藕miemy pod uwag takie rzeczy jak: dugi prompt skadajcy si z komplikowanej struktury, few-shot examples, treci dokumentu, kt贸ry chcemy analizowa i dodamy do tego odpowied藕 LLM, szybko zdamy sobie spraw, 偶e to wcale nie jest tak du偶o. W tym pocie zagbimy si w ten temat, budujc prosty keyword extractor z u偶yciem GPT-4o-mini i Node.js.

## Dlaczego zarzdzanie context window jest wa偶ne?

Stabilno i przewidywalno to tym wypadku jedne z najwa偶niejszych aspekt贸w. Nawet jeli kontekst modelu jest du偶y, API mo偶e mie ograniczenia w liczbie token贸w na minut. Dokadno odpowiedzi modelu i jego wydajno r贸wnie偶 s kluczowe. Wysyanie caego tekstu na raz do analizy nie jest najlepszym pomysem, wynik prawdopodobnie bdzie gorszy, poniewa偶 model bdzie musia skupi si na szerszym kontekcie. Wysyanie wikszej liczby zapyta r贸wnolegle pozwoli na szybsz analiz i wy偶sz jako odpowiedzi. Majc te aspekty na uwadze, mo偶emy dostosowa rozmiar i liczb zapyta wysyanych do modelu, aby speniay wymagania aplikacji i mieciy si w limitach.

## Co to s tokeny?

Token to najmniejsza jednostka danych tekstowych. Sowo jest dzielone na fragmenty. Proces dzielenia tekstu obsugiwany jest przez tokenizer. Spos贸b tokenizacji tekstu r贸偶ni si w zale偶noci od LLM i jego tokenizera. Wicej na temat token贸w znajdziesz [w tym artykule OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them).

## Co to jest context window?

Context window odnosi si do liczby token贸w, kt贸re LLM mo偶e obsugiwa jednoczenie. Jest to co w rodzaju pamici. Wyobra藕 sobie, 偶e piszesz streszczenie ksi偶ki. Jeste w stanie przetworzy tylko 50 stron na raz, reszta zostaje zapomniana. Tak wanie dziaa context window, szczeg贸lnie w przypadku keyword extractor'a, kt贸ry zamierzamy stworzy. W naszym przypadku skupimy si na jednej wiadomoci, kt贸ra skada si z: promptu, schem'y i tekstu do analizy. Takie podejcie wymaga innych strategii ni偶 implementacja konwersacji(chat), jak np. prompt-odpowied藕.

## Budowanie keyword extractor'a

Celem naszej aplikacji jest ekstrakcja s贸w kluczowych z tekstu, mo偶e to by artyku, ksi偶ka, strona internetowa itp.. Musimy by wic przygotowani na bardzo dugie teksty, dlatego zarzdzanie context window jest kluczowe.

### Strategie zarzdzania context window

Aby skutecznie speni wymagania naszej aplikacji i zapewni jej stabilno, musimy zastosowa nastpujce strategie zarzdzania context widnow:

- **Strategie promptowania** Mo偶emy kontrolowa wynik, dodajc wytyczne w promptcie. W naszym przypadku mo偶emy poinstruowa LLM, aby zwraca tylko 5 najbardziej istotnych fraz kluczowych skadajcych si z maksymalnie 2-3 s贸w. Jest to wa偶ne, poniewa偶 API OpenAI nie pozwala na zamieszczanie dopuszczalnej iloci w schema.

- **Liczenie token贸w** Aby policzy tokeny, bdziemy musieli wybra tokenizer i zaimplementowa logik liczenia przed wysaniem tekstu do LLM. Ponadto bdziemy musieli wyestymowa, ile token贸w wyniesie odpowied藕.

- **Porcjowanie tekstu(chunking)** Dzielenie tekstu na fragmenty upewni nas, 偶e nie przekroczymy context window. W naszym przypadku potrzeba analizy szerokiego kontesktu jest mniej istotna, wic mo偶emy zastosowa bardziej agresywne strategie porcjowaniawysyajc wicej, ale mniejszych fragment贸w do LLM. Pamitajc o tym 偶eby sowa/zdania nie byy pocite.

- **Structured output:** Wymuszenie na LLM generowania danych w ustrukturyzowanym formacie oraz ustawienie `temperature` na `0` w celu uzyskania bardziej skupionego wyniku daje nam r贸wnie偶 kontrol nad context window poniewa偶 mo偶emy atwiej oszacowa, ile token贸w bdzie mia wynik

### Bierzemy si do pracy

Pierwsz i najwa偶niejsz rzecz, kt贸r musimy mie przygotowan, s prompt'y:

```typescript
export const systemPrompt = `You are a specialized AI assistant focused on keyword extraction from blog posts.
Your primary role is to identify and extract the most relevant keywords and key phrases that best represent the content.
Always provide keywords in a consistent format and focus on technical accuracy.`;

export const mainPrompt = `Analyze the following blog post and extract the most relevant keywords and key phrases.
Guidelines:
- Extract 5 most important keywords/phrases
- Focus on technical terms, concepts, and specific terminology
- Include both single words and short phrases when relevant (2-3 words)
- Exclude common stop words unless part of a specific term
- Add confident scores to each keyword/phrase
Input text:`;
```

Jak wspomniaem wczeniej, m贸wic o **strategiach promptowania**, mo偶emy wyra藕nie powiedzie LLM, jak duga powinna by lista s贸w kluczowych i jak dugie mog by poszczeg贸lne sowa kluczowe.

Koczymy nasz g贸wny prompt tekstem `Input text:`, poniewa偶 p贸藕niej w kodzie poczymy tekst do analizy z g贸wnym promptem.

Gdy mamy ju偶 przygotowane prompty, czas zdefiniowa nasz schemat dla **structured output**. Do obsugi schem'y u偶yjemy `zod'a`.

```typescript
import { z } from "zod";

export const keywordsResponseSchema = z.object({
  results: z.array(z.object({ keyword: z.string(), confidence: z.number() })),
});
```

`zod` doskonale wsp贸pracuje z metod: `withStructuredOutput` z biblioteki `langchain`, kt贸rej bdziemy u偶ywa do zarzdzania zapytaniami API LLM. `langchain` konwertuje `zod` schema na `JSON` schema akceptowany przez OpenAI API. Dodatkowo mo偶emy skorzysta z typ贸w dla naszych wynik贸w, kt贸re s automatycznie wnioskowane ze schem'y.

### Porcjowanie tesktu i liczenie token贸w

W naszym przykadzie rozmiar fragmentu to 2000 token贸w. Context window modelu GPT-4o-mini wynosi 128k token贸w, teoretycznie moglibymy ustawi rozmiar fragmentu bliski tej wartoci, ale mniejszy sprawdzi si lepiej pod wzgldem wydajnoci i jakoci wynik贸w.

Zadeklarujmy nasz funkcj w ten spos贸b:

```typescript
async function getChunkedText(content: string, maxChunkSize: number = 2000) {
  /* --- */
}
```

Pierwsz rzecz, kt贸r musimy zrobi, aby okreli, jak podzieli tekst, s obliczenia. Do tego potrzebujemy tokenizera, kt贸ry zamieni tekst na tokeny. W naszym przypadku `js-tiktoken` jest najlepszym wyborem, poniewa偶 jest kompatybilny z modelem, kt贸rego aktualnie u偶ywamy.

```typescript
import { encodingForModel } from "js-tiktoken";

async function getChunkedText(content: string, maxChunkSize: number = 2000) {
  const encoder = encodingForModel("gpt-4o-mini-2024-07-18");

  /* --- */
}
```

Aby nasze estymacje byy dokadne, musimy odj tokeny u偶ywane przez prompty, schema i tekst od parametru `maxChunkSize`. Nasza schema jest w formacie `zodType`, wic musimy go przekonwertowa na string. Do tego u偶ywamy paczki `zod-to-json-schema`.

```typescript
import { encodingForModel } from "js-tiktoken";
import { mainPrompt, systemPrompt } from "./prompts";
import zodToJsonSchema from "zod-to-json-schema";
import { keywordsResponseSchema } from "./schema";
import { type ZodType } from "zod";

function getSchemaAsString(schema: ZodType<any>) {
  const jsonSchema = zodToJsonSchema(schema);

  const schemaString = JSON.stringify(jsonSchema);

  return schemaString;
}

async function getChunkedText(content: string, maxChunkSize: number = 2000) {
  const encoder = encodingForModel("gpt-4o-mini-2024-07-18");
  const schemaString = getSchemaAsString(keywordsResponseSchema);

  const calculatedChunkSize =
    maxChunkSize -
    encoder.encode(systemPrompt).length -
    encoder.encode(mainPrompt).length -
    encoder.encode(schemaString).length;

  /* --- */
}
```

W wyniku oblicze otrzymujemy ostateczny rozmiar fragmentu. Do konstruktora splittera przekazujemy dwie wartoci:

- `chunkSize:` Obliczony rozmiar fragmentu tekstu.
- `lengthFunction:` Funkcja, kt贸ra **liczy tokeny** i jest mechanizmem decyzyjnym w procesie podziau tekstu przez splitter.

W naszej aplikacji u偶ywamy `RecursiveCharacterTextSplitter`, kt贸ry dzieli tekst od zda do s贸w. Oznacza to, 偶e algorytm najpierw sprawdza, czy jedna lub wicej fraz mieci si w obliczonym fragmencie tekstu. Jeli nie, rekurencyjnie dzieli tekst na mniejsze czci.

```typescript
import { encodingForModel, Tiktoken } from "js-tiktoken";
import { mainPrompt, systemPrompt } from "./prompts";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import zodToJsonSchema from "zod-to-json-schema";
import { keywordsResponseSchema } from "./schema";
import { type ZodType } from "zod";

function getSchemaAsString(schema: ZodType<any>) {
  const jsonSchema = zodToJsonSchema(schema);

  const schemaString = JSON.stringify(jsonSchema);

  return schemaString;
}

async function getChunkedText(content: string, maxChunkSize: number = 2000) {
  const encoder = encodingForModel("gpt-4o-mini-2024-07-18");
  const schemaString = getSchemaAsString(keywordsResponseSchema);

  const calculatedChunkSize =
    maxChunkSize -
    encoder.encode(systemPrompt).length -
    encoder.encode(mainPrompt).length -
    encoder.encode(schemaString).length;

  handleLimitError({ calculatedChunkSize, schemaString }, encoder);

  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: calculatedChunkSize,

    lengthFunction: (text: string) => encoder.encode(text).length,
  });
  const texts = await textSplitter.splitText(content);

  return texts;
}

export default getChunkedText;
```

### Wysyanie zapyta i odpowiedzi zwr贸conych z api LLM

Mamy ju偶 funkcj do dzielenia tekstu, dziki czemu mo偶emy zacz wysya zapytania do modelu.

Pierwsz rzecz jest konfiguracja. Korzystamy z integracji `langchain` z OpenAI.

Warto wspomnie, 偶e ustawienie `temperature` na `0` pozwala nam porednio wpyn na odpowied藕 LLM, czynic wynik bardziej zwizym zapobiegajc nadmiernej kreatywnoci modelu.

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4o-mini-2024-07-18",
  temperature: 0,
  apiKey: process.env.OPENAI_API_KEY,
});

/* --- */
```

Nastpnie deklarujemy funkcj do wywoywania LLM. Musimy przetworzy nasze dane i prompty, wic adujemy dokument z tekstem i dzielimy go na fragmenty za pomoc wczeniej przygotowanej funkcji.

Na potrzeby naszego przykadu adujemy plik z tekstem, m贸gby on by r贸wnie偶 przekazany do funkcji jako argument.

```typescript
export async function callLlm() {
  const text = loadTextFromFile();
  const textChunks = await getChunkedText(text);
  /*  ---  */
}
```

Teraz mo偶emy zarejestrowa `zod` schem'e, aby ustawi structured output, i wysa zapytania. Mo偶emy wykorzysta fakt, 偶e tekst jest podzielony i wysya zapytania r贸wnolegle apic odpowiedzi z API pomocy `Promise.all`. Pozwala to na przetwarzanie danych szybciej w mniejszych czciach, co pomaga LLM skupi si na szczeg贸ach i daje lepsze wyniki. Pamitaj jednak, 偶e wysyajc 偶dania w ten spos贸b, musisz uwzgldni rate limity API.

```typescript
export async function callLlm() {
  const text = loadTextFromFile();
  const textChunks = await getChunkedText(text);
  const structured = model.withStructuredOutput(keywordsResponseSchema);
  const completions = await Promise.all(
    textChunks.map(async (chunk) => {
      return structured.invoke([
        {
          role: "system",
          content: systemPrompt,
        },
        {
          role: "user",
          content: mainPrompt + chunk,
        },
      ]);
    }),
  );

  /* --- */
}
```

Ostatnim krokiem jest formatowanie wynik贸w i filtrowanie danych. Scalemy odpowiedzi z poszczeg贸lnych fragment贸w i filtrujemy je na podstawie wartocu confidence, aby upewni si, 偶e zwracane s tylko odpowiednie sowa kluczowe. Na kocu usuwamy duplikaty za pomoc `new Set()`.

```typescript
export async function callLlm() {
  const text = loadTextFromFile();
  const textChunks = await getChunkedText(text);
  const structured = model.withStructuredOutput(keywordsResponseSchema);
  const completions = await Promise.all(
    textChunks.map(async (chunk, index) => {
      return structured.invoke([
        {
          role: "system",
          content: systemPrompt,
        },
        {
          role: "user",
          content: mainPrompt + chunk,
        },
      ]);
    }),
  );

  const results = completions
    .flatMap((completion) => completion.results)
    .filter((result) => result.confidence >= 0.85)
    .map((result) => result.keyword);
  return [...new Set(results)];
}
```

Finalna odpowied藕:
![log wyniku](/posts/assets/managing-the-context-window-of-gtp-4o-mini-in-javascript/result-log.png)

## Podsumowanie

Zarzdzanie context window jest kluczowe, szczeg贸lnie gdy skupiamy si na analizie du偶ych tekst贸w. Rate limity api mog by problematyczne, mimo 偶e context window wynosi 128k token贸w, rate limity mog utrudni jego wykorzystanie w peni. Na przykad, obecnie dla GPT-4o-mini hostowanego na Azure Open AI maksymalny pocztkowy limit to 30k token贸w na minut! Aby uzyska wicej, trzeba poprosi Microsoft o wikszy limit. Nawet jeli Twoje limity s wy偶sze, utrzymanie kontroli nad context window jest niezbdne, zar贸wno w przypadku aplikacji produkcyjnej, jak i przy projektach hobbystycznych. Nikt nie chce uruchamia swoich integracji AI, cigle napotykajc bdy.

**Dzieki za przeczytanie **

PS: Zerknij na repo keword extractora [tutaj na GitHub'ie](https://github.com/greg2012201/keyword-extractor)
